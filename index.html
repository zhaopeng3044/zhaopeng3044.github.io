<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-AdaBoost" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/08/AdaBoost/" class="article-date">
  <time datetime="2018-11-08T06:51:04.000Z" itemprop="datePublished">2018-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/08/AdaBoost/">AdaBoost</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h2 id="提升方法的基本思想"><a href="#提升方法的基本思想" class="headerlink" title="提升方法的基本思想"></a>提升方法的基本思想</h2><p>提升方法(Boosting)基于这样一种思想:对于一个复杂的任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断要好.实际上就是三个臭皮匠顶过一个诸葛亮.<br>历史上,Kearns和 Valiant首先提出了”强可学习(Strong learnable)”和弱可学习(Weakly learnable)”的概念.指出:在概率近似正确(probably approximately correct, PAC)学习框架中,一个概念(一个类),如果存在一个多项式的学习算法能够完成它,并且正确率很高,那么就称这个概念是强可学习的.一个概念,如果存在一个多项式的学习算法能够学习它,学习的正确率仅比随机猜略好,那么称这个概念是弱可学习的.后来Schapire证明强可学习与弱可学习是等价的.也就是说,在PAC学习框架下,一个概念是强可学习的充分必要条件是这个概念是弱可学习的.这样一来,问题便成为,在学习中,如果已经发现了”弱可学习算法”,那么能否将它提升(Boost)为”强可学习算法”.大家知道,发现弱可学习算法,通常要比发现强可学习算法容易的多.那么如何具体实施提升,便成为开发提升方法时所要解决的问题.关于提升方法的研究有很多,有很多方法被提出.最具代表性的就是AdaBoost算法(AdaBoost algorithm).</p>
<h2 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h2><p>现在叙述AdaBoost算法,假设给定一个二分类的训练数据集</p>
<script type="math/tex; mode=display">T=\left \{ (x_{1}, y_{1}), (x_{2}, y_{2}), ...,(x_{N}, y_{N}) \right \}</script><p>其中,每个样本点由实例与标记组成.实例<script type="math/tex">x_{i}\in \chi \subseteq R^n</script>,标记<script type="math/tex">y_{i}\in \gamma =\left \{ -1, 1 \right \}</script>, <script type="math/tex">\chi</script>是实例空间,<script type="math/tex">\gamma</script>是标记集合.AdaBoost利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器<em>线性</em>组合成为一个强分类器。</p>
<h3 id="算法1-AdaBoost"><a href="#算法1-AdaBoost" class="headerlink" title="算法1(AdaBoost)"></a>算法1(AdaBoost)</h3><p>输入：训练数据集<script type="math/tex">T=\left \{ (x_{1}, y_{1}), (x_{2}, y_{2}), ...,(x_{N}, y_{N}) \right \}</script>,其中<script type="math/tex">x_{i}\in \chi \subseteq R^n</script>，<script type="math/tex">\gamma \in \left \{ -1, +1 \right \}</script>;弱学习算法：<br>输出：最终分类器G(x)。</p>
<ul>
<li>(1)初始化训练数据的权值分布 <script type="math/tex; mode=display">D_{1} = (w_{11},...,w_{1i},...,w{iN}), w_{1i}=\frac{1}{N}, i=1,2,...,N</script></li>
<li>(2)对m=1,2,…M(m代表迭代次数，或弱分类器的个数)<ul>
<li>(a)使用具有权值分布<script type="math/tex">D_{m}</script>的训练数据集学习，得到基本分类器<script type="math/tex; mode=display">G_{m}(x): \chi\rightarrow \left \{ -1,+1 \right \}</script></li>
<li>(b)计算<script type="math/tex">G_{m}(x)</script>在训练数据集上的分类误差率<script type="math/tex; mode=display">e_{m} = \sum _{i=1}^{N} P(G_{m}(x_{i})\neq y_{i})=\sum _{i=1}^{N}w_{mi}I(G_{m}(x_{i})\neq y_{i})</script></li>
<li>(c)计算<script type="math/tex">G_{m}(x)</script>的系数<script type="math/tex; mode=display">\alpha _{m} = \frac{1}{2}\log \frac {1-e_{m}}{e_{m}}</script>这里的对数是自然对数</li>
<li>(d)更新训练数据集的权值分布<script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1,i} = \frac {w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))</script>这里，<script type="math/tex">Z_{m}</script>是规范化因子<script type="math/tex; mode=display">Z_{m} = \sum _{i=1}^{N} w_{mi}exp(-\alpha _{m} y_{i} G_{m}x_{i})</script>它使<script type="math/tex">D_{m+1}</script>成为一个概率分布，<script type="math/tex">\sum _{i}^{N} w_{m+1,i}=1</script></li>
</ul>
</li>
<li>(3)构建基本分类器的线性组合<script type="math/tex; mode=display">f(x) = \sum _{m=1}^{M} \alpha_{m}G_{m}(x)</script>得到最终的分类器<script type="math/tex; mode=display">G(x)=sign(f(x))=sign( \sum _{m=1}^{M} \alpha_{m}G_{m}(x))</script></li>
</ul>
<h2 id="AdaBoost算法的训练误差分析"><a href="#AdaBoost算法的训练误差分析" class="headerlink" title="AdaBoost算法的训练误差分析"></a>AdaBoost算法的训练误差分析</h2><p>AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差，关于这个问题有下面的定理：</p>
<h3 id="定理1-AdaBoost算法最终分类器的训练误差上界为"><a href="#定理1-AdaBoost算法最终分类器的训练误差上界为" class="headerlink" title="定理1 AdaBoost算法最终分类器的训练误差上界为"></a>定理1 AdaBoost算法最终分类器的训练误差上界为</h3><script type="math/tex; mode=display">\frac {1}{N}\sum _{i=1}^{N}I(G(x_{i})\neq y_{i})\leq \frac {1}{N} \sum _{i}exp(-y_{i}f(x_{i}))=\prod _{m}Z_{m}</script><p>这里G(x), f(x)和<script type="math/tex">Z_{m}</script>有上面的式子可以得到。<br>    证明： 当<script type="math/tex">G(x_{i}) \neq y_{i}</script>时，<script type="math/tex">y_{i}f(x_{i}) < 0</script>,因此<script type="math/tex">exp(-y_{i}f(x_{i})) \geqslant 1</script>由此可以直接证明前半部分。<br>    后半部分的推导要用到<script type="math/tex">Z_{m}</script>的定义式以及更新<script type="math/tex">w_{m+1,i}</script>变形式</p>
<script type="math/tex; mode=display">w_{m,i}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))=Z_{m}w_{m+1,i}</script><p>推导如下:<br>\begin{align}<br>\frac{1}{N}\sum _{i}exp(-y_{i}f(x_{i}))\\\\<br>=\frac {1}{N}\sum _{i}(exp(-\sum _{m=1}^{M} \alpha_{m}y_{i}G_{m}(x_{i})))\\\\<br>=\sum _{i} w_{1i} \prod _{m=1}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{i} w_{1i} exp(-\alpha _{1} y_{i} G_{1}(x_{i})) \prod _{m=2}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{i} Z_{1} w_{2i} \prod _{m=2}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=Z_{1} \sum _{i} w_{2i} exp(-\alpha _{2} y_{i} G_{2}(x_{i})) \prod _{m=3}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=Z_{1} Z_{2} \sum _{i} w_{3i} exp(-\alpha _{3} y_{i} G_{3}(x_{i})) \prod _{m=4}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=…\\\\<br>=Z_{1} Z_{2} … Z_{M} \sum _{i} w_{m+1,i}\\\\<br>=\prod _{m=1}^{M} Z_{m}\\\\<br>\end{align}<br>这一定理说明，可以在每一轮选取适当的<script type="math/tex">G_{m}</script>使得<script type="math/tex">Z_{m}</script>最小，从而使训练误差下降最快。</p>
<h3 id="二分类问题AdaBoost的训练误差界"><a href="#二分类问题AdaBoost的训练误差界" class="headerlink" title="二分类问题AdaBoost的训练误差界"></a>二分类问题AdaBoost的训练误差界</h3><script type="math/tex; mode=display">\prod _{m=1}^{M} Z_{m}= \prod _{m=1}^{M}(2\sqrt {e_{m}(1-e_{m})})=\prod _{m=1}^{M} \sqrt {(1-4\gamma _{m}^{2})} \leq exp(-2\sum _{m=1}^{M} \gamma _{m}^{2})</script><p>这里，<script type="math/tex">\gamma _{m}=\frac{1}{2}-e_{m}</script></p>
<script type="math/tex; mode=display">\alpha _{m} = \frac{1}{2} \log \frac {1-e_{m}}{e_{m}}</script><p>\begin{align}<br>Z_{m}=\sum _{i=1}^{N} w_{mi} exp(-\alpha -{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{y_{i}=G_{m}(x_{i})} w_{mi}e^{-\alpha_{m}} + \sum _{y_{i} \neq G_{m}(x_{i})} w_{mi}e^{\alpha_{m}}\\\\<br>=(1-e_{m})e^{-\alpha_{m}}+e_{m}e^{\alpha_{m}}\\\\<br>=2 \sqrt{e_{m}(1-e_{m})} = \sqrt {1-4\gamma_{m}^{2}}<br>\end{align}<br>至于不等式:</p>
<script type="math/tex; mode=display">\prod _{m=1}^{M} \sqrt{(1-4 \gamma_{m}^{2})} \leq exp(-2 \sum _{m=1}^{M} \gamma _{m}^{2})</script><p>则可先由<script type="math/tex">e^{x}</script>和<script type="math/tex">\sqrt{1-x}</script>在点x=0的泰勒展开式推出不等式<script type="math/tex">\sqrt{(1-4\gamma_{m}^{2})} \leq exp(-2 \gamma _{m}^{2})</script></p>
<h2 id="AdaBoost算法的解释"><a href="#AdaBoost算法的解释" class="headerlink" title="AdaBoost算法的解释"></a>AdaBoost算法的解释</h2><p>AdaBoost算法还有另一个解释,即可以认为AdaBoost算法是模型为加法模型,损失函数为指数函数,学习算法为前向分步算法时的二分类学习方法.</p>
<h3 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h3><p>考虑加法模型(additive model)</p>
<script type="math/tex; mode=display">f(x)=\sum _{m=1}^{M} \beta _{m} b(x;\gamma_{m})</script><p>其中,<script type="math/tex">b(x;\gamma _{m})</script>为基函数,<script type="math/tex">\gamma _{m}</script>为基函数的系数.显然AdaBoost是一个加法模型<br>在给定训练数据集以及损失函数L(y,f(x))的条件下,学习加法模型f(x)成为经验风险极小化即损失函数极小化问题:</p>
<script type="math/tex; mode=display">\underset{\beta _{m}, \gamma _{m}}{\text{min}} \sum _{m=1}^{N} L(y_{i}, \sum _{m=1}^{M} \beta_{m}b(x_{i};\gamma_{m}))</script><p>通常这是一个复杂的优化问题,前向分步算法(forward stagewise algorithm)求解这一优化问题的想法是:因为学习模型是加法模型,如果能够从前向后,每一步只学习一个基函数及其系数,逐步逼近优化目标函数,那么就可以简化优化的复杂度.具体,每步只需要优化如下损失函数:</p>
<script type="math/tex; mode=display">\underset{\beta , \gamma } \sum _{i=1}^{N} L(y_{i}, \beta b(x_{i};\gamma))</script>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/08/AdaBoost/" data-id="cjo9iw26z0000iskuc9g7wgy9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-信息熵" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/19/信息熵/" class="article-date">
  <time datetime="2018-10-20T02:51:00.000Z" itemprop="datePublished">2018-10-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/19/信息熵/">信息熵</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近组内小伙伴峰哥分享了熵在机器学习中的应用，所以写下这篇blog，记录一下，在此再次感谢蜂哥。</p>
<h2 id="熵在物理学中的应用"><a href="#熵在物理学中的应用" class="headerlink" title="熵在物理学中的应用"></a>熵在物理学中的应用</h2><p>熵最初来自于热力学，用来表示能量在空间分布的混乱程度，简而言之熵越大，系统越混乱，越难预测。当一个系统的能量达到完全平均分布时，这个系统的熵达到最大。</p>
<h2 id="自信息量"><a href="#自信息量" class="headerlink" title="自信息量"></a>自信息量</h2><p>自信息是熵的基础，它表示某一事件<strong>发生时</strong>所带来的信息量的多少，当该事件发生的概率越大，其自信息越小。比如太阳从东方升起，该事件发生时，它的自信息就很少，因为每个人都知道该时间一定会发生。而中国足球国家队在下一届世界杯夺冠，该事件发生时，信息量则很大，该时间发生时，可能包涵着中国国家队翻天覆地的变化。<br>那如何度量熵呢？我们需要找到一个函数，来量化熵的值，它需要满足一下条件</p>
<ul>
<li>自信息量是一个非负数：一个时间的发生要么能给我们带来正的信息量（增加我们的信息）；要么没有给我们带来任何信息，信息量为零；但绝不会从我们这偷走信息。所以自信息是非负数。</li>
<li>自信息跟概率有关系：当某一事件是极不可能出现时（发生的概率很小），当它确实发生时，会给我们带来巨大的信息量；当某一事件一定发生时，它发生时，给我们带来的信息量为零。因此信息量应该依赖于概率分布<em>P</em>(X),所以说自信息量<em>h(x)</em>定义为概率<em>p(x)</em>单调递减函数。</li>
<li>自信息量之间是可以叠加的：假设两个事件<em>x</em>和<em>y</em>是相互独立的，那个两个事件同时发生的概率为 <em>p(x,y)</em>=<em>p(x)</em> <em> </em>p(y)<em>，那么分别观测这两个变量得到的信息量和同时观测到这两个变量的信息量是相同的，即： </em>h(x,y)=h(x)+h(y)<em>, 所以此处可以得出自信息量的定义 </em>h(x)<em> 应该是概率 </em>p(x)<em>的</em>log* 函数，同时保证自信息量非负。则自信息量的定义为：<script type="math/tex">H(x)=\log (-\frac{1}{p(x)})=-\log p(x)</script> <img src="https://pic2.zhimg.com/80/v2-5366c37ae1f87ca30e0d58a3bd76e4f5_hd.jpg" alt="image"></li>
</ul>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>前面提到的自信息是某一事件发生时，所带来的信息量的大小。在信息论中，熵是接受每条消息中包含信息的×<strong>平均量</strong>,又被称为信息熵，平均信息量。这里的消息代表来自分布或数据流中的事件，样本或特征。熵可以理解为不确定性的度量。熵的大小跟事件不确定性有关，不确定性的变化跟如下两个方面有关：</p>
<ul>
<li>事件可能结果的数量有关：当可能结果数量比较大时，我们得到新信息才有潜力拥有大信息量。</li>
<li>跟概率有关：单看事件可能发生的不同结果数量是不够的，还要看事件不同结果的概率分布。比如2018-19赛季，NBA西部冠军的结果有15种可能，但是有95%的概率是勇士获得西部决赛的冠军。因此该事件所具有的信息熵也比较小。<br><strong>熵的大小可能跟结果的数量有关</strong>: 用熵来评价整个随机变量 <em>x</em>的平均信息量，而平均信息量最好的量度就是随机变量的期望，就是随机变量<em>x</em>的所有取值对应的自信息量的平均值，所以最终熵的定义如下：<br><img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=\Large H%28x%29 = \sum_{x}p%28x%29h%28x%29=-\sum_{x}p%28x%29log_2p%28x%29" alt="image"><br><em>log</em>函数的基可以是任意的，在计算机领域计算熵基本以2为底，因此信息熵的单位是bits;而在机器学习中，基数常选择自然数e，因此单位被称为nats。<strong>熵是有单位的</strong>x.</li>
</ul>
<h2 id="信息熵在编码当中的应用"><a href="#信息熵在编码当中的应用" class="headerlink" title="信息熵在编码当中的应用"></a>信息熵在编码当中的应用</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">事件(X)</th>
<th style="text-align:right">概率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">A</td>
<td style="text-align:right">1/2</td>
</tr>
<tr>
<td style="text-align:left">B</td>
<td style="text-align:right">1/4</td>
</tr>
<tr>
<td style="text-align:left">C</td>
<td style="text-align:right">1/8</td>
</tr>
<tr>
<td style="text-align:left">D</td>
<td style="text-align:right">1/8</td>
</tr>
</tbody>
</table>
</div>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="math/tex; mode=display">H(x)=-(\frac{1}{2}\times\log_{2}(\frac{1}{2})+\frac{1}{4}\times\log_{2}(\frac{1}{4})+\frac{1}{8}\times\log_{2}(\frac{1}{8})+\frac{1}{8}\times\log_{2}(\frac{1}{8})</script><p>H(x)=1.75,表示上例中随机事件X平均需要1.75个bit来编码。通过信息熵可以确定随机变量X的最优编码的平均编码长度，但无法生成最终的编码形式。想要得到最优的编码结果需要借助霍夫曼编码（Huffman Coding）。在这里通过霍夫曼编码可以得到:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">事件(X)</th>
<th style="text-align:right">编码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">A</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">B</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:left">C</td>
<td style="text-align:right">110</td>
</tr>
<tr>
<td style="text-align:left">D</td>
<td style="text-align:right">111</td>
</tr>
</tbody>
</table>
</div>
<p>假设随机变量<em>X</em>实验8次得到8次结果，A事件发生4次，B事件发生2次，C事件发生1次，D事件发生1次，总共编码长度为 <script type="math/tex">1\times4+2\times2+3\times1+3\times1=14</script><br>平均每个事件编码长度为<script type="math/tex">\frac{14}{8}=1.75</script></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>现在关于样本集的两个概率分布<em>p</em>和<em>q</em>,其中<em>p</em>是样本的真实分布，<em>q</em>非真实分布。按照真实分布<em>p</em>来衡量识别一个样本的所需要的编码长度的期望（平均编码长度）为<script type="math/tex">H(p)=-\sum_{i}p(i)\times\log_{2}p(i)</script>。如果使用错误的<em>q</em>来表示真实分布<em>p</em>的平均编码长度，则应该是：<script type="math/tex">H(p,q)=-\sum_{i}p(i)\times\log_{2}q(i)</script>。因为用<em>q</em>来编码的样本来自分布<em>p</em>，所以期望<em>H</em>(<em>p</em>,<em>q</em>)中用的概率是<em>p</em>(i)。<br>在逻辑回归中的Sigmoid和softmax函数中交叉熵作为损失函数使用。其中主要用于度量两个概率分布间的差异信息。<em>p</em>对<em>q</em>的交叉熵表示<em>q</em>分布的自信息对<em>p</em>分布的期望，公式定义为 ：<script type="math/tex">H(p,q)=-\sum_{x}p(x)\times\log_{2}q(x)</script><br>其中，<em>p</em>是真实或现实中样本的分布（通常都是未知的），<em>q</em>是预测得到的样本分布。在信息论中，交叉熵的计算数值表示：如果用错误的编码方式<em>q</em>去编码真实分布<em>p</em>的事件，需要多少bit位，是一种非常有用的衡量概率分布相似性的数学工具。由于交叉熵在逻辑回归中应用广泛，其定义为：</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i}^{m}(y_{i}\log_{2}h_{\theta}(x_{i})+(1-y_{i})\log_{2}(1-h_{\theta}(x_{i})))</script><p>其中<script type="math/tex">y_{i}</script>是第<em>i</em>个样本的真实标签，<em>h</em>是sigmoid预测输出值。比如真实的分布是该样本属于正类，即<script type="math/tex">y_{i}</script>为1,模型预测出来的数据正类的概率<script type="math/tex">h_{\theta}(x_{i})</script>为0.8，则损失为<script type="math/tex">1\times\log_{2}(0.8)</script>。在逻辑回归模型中，样本<script type="math/tex">y_{i}</script>属于正类(1)的概率要么为0,要么为1,所以<script type="math/tex">J(\theta)</script> 可以看作是我们通过机器学习学到的分布<em>q</em>来拟合真实分布<em>p**，当</em>q<em>越逼近真实分布</em>p*时，<script type="math/tex">J(\theta)</script>值越小。所以逻辑回归的损失函数可以从极大似然估计与熵原理两个角度解读。</p>
<h2 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h2><p>我们由<em>q</em>得到的平均编码长度比<em>p</em>得到的平均编码长度多出的bit数称为<strong>相对熵</strong>：相对熵又称为互熵，Kullback熵，Kullback-Leible散度(即KL散度)等。<em>p</em>是真实分布，<em>q</em>是预测得到的样本分布，则<em>q</em>对<em>p</em>的相对熵为：</p>
<script type="math/tex; mode=display">HK(p||q)=H(p,q)-H(p)=-\sum_{x}(p(x)(\log q(x)-\log q(x))) = -\sum_{x}p(x)\log \frac{q(x)}{p(x)}</script><p><strong>为什么真实分布<em>p</em>才能得到最优编码</strong><br>我们可以从相对熵的角度进行解释，对于一个真实分布<em>p</em>,使用真实分布自身的编码获得的编码长度是最小的，用任何一种其他分布的<em>q</em>去编码真实分布<em>p</em>，都会使得编码长度变大，只需要证明相对熵<em>KL(p||q)</em>非负就可以了。为了证明如下等式成立，我们用到了数学中的一个不等式：<script type="math/tex">\log(x)\leq x-1</script>,当且仅当<em>x</em>=1等式成立，因此：</p>
<script type="math/tex; mode=display">-\sum_{x}p(x)\log \frac{q(x)}{p(x)} \geq \sum_{x}p(x)(1-\frac{q(x)}{p(x)})=\sum_{x}(p(x)-q(x))=\sum_{x}p(x)-\sum_{x}q(x)=1-1=0</script>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/19/信息熵/" data-id="cjo9iw27e0002iskuwno07ehs" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/10/hello-world/" class="article-date">
  <time datetime="2018-10-10T05:33:05.260Z" itemprop="datePublished">2018-10-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/10/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/10/hello-world/" data-id="cjo9iw27a0001iskuy0xy24rd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/08/AdaBoost/">AdaBoost</a>
          </li>
        
          <li>
            <a href="/2018/10/19/信息熵/">信息熵</a>
          </li>
        
          <li>
            <a href="/2018/10/10/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>