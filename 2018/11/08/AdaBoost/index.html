<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>AdaBoost | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="提升方法的基本思想提升方法(Boosting)基于这样一种思想:对于一个复杂的任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断要好.实际上就是三个臭皮匠顶过一个诸葛亮.历史上,Kearns和 Valiant首先提出了”强可学习(Strong learnable)”和弱可学习(Weakly learnable)”的概念.指出:在概率近似正确(probably">
<meta property="og:type" content="article">
<meta property="og:title" content="AdaBoost">
<meta property="og:url" content="http://yoursite.com/2018/11/08/AdaBoost/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="提升方法的基本思想提升方法(Boosting)基于这样一种思想:对于一个复杂的任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断要好.实际上就是三个臭皮匠顶过一个诸葛亮.历史上,Kearns和 Valiant首先提出了”强可学习(Strong learnable)”和弱可学习(Weakly learnable)”的概念.指出:在概率近似正确(probably">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-09T08:59:01.017Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AdaBoost">
<meta name="twitter:description" content="提升方法的基本思想提升方法(Boosting)基于这样一种思想:对于一个复杂的任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断要好.实际上就是三个臭皮匠顶过一个诸葛亮.历史上,Kearns和 Valiant首先提出了”强可学习(Strong learnable)”和弱可学习(Weakly learnable)”的概念.指出:在概率近似正确(probably">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-AdaBoost" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/08/AdaBoost/" class="article-date">
  <time datetime="2018-11-08T06:51:04.000Z" itemprop="datePublished">2018-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      AdaBoost
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h2 id="提升方法的基本思想"><a href="#提升方法的基本思想" class="headerlink" title="提升方法的基本思想"></a>提升方法的基本思想</h2><p>提升方法(Boosting)基于这样一种思想:对于一个复杂的任务来说,将多个专家的判断进行适当的综合所得出的判断,要比其中任何一个专家单独的判断要好.实际上就是三个臭皮匠顶过一个诸葛亮.<br>历史上,Kearns和 Valiant首先提出了”强可学习(Strong learnable)”和弱可学习(Weakly learnable)”的概念.指出:在概率近似正确(probably approximately correct, PAC)学习框架中,一个概念(一个类),如果存在一个多项式的学习算法能够完成它,并且正确率很高,那么就称这个概念是强可学习的.一个概念,如果存在一个多项式的学习算法能够学习它,学习的正确率仅比随机猜略好,那么称这个概念是弱可学习的.后来Schapire证明强可学习与弱可学习是等价的.也就是说,在PAC学习框架下,一个概念是强可学习的充分必要条件是这个概念是弱可学习的.这样一来,问题便成为,在学习中,如果已经发现了”弱可学习算法”,那么能否将它提升(Boost)为”强可学习算法”.大家知道,发现弱可学习算法,通常要比发现强可学习算法容易的多.那么如何具体实施提升,便成为开发提升方法时所要解决的问题.关于提升方法的研究有很多,有很多方法被提出.最具代表性的就是AdaBoost算法(AdaBoost algorithm).</p>
<h2 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h2><p>现在叙述AdaBoost算法,假设给定一个二分类的训练数据集</p>
<script type="math/tex; mode=display">T=\left \{ (x_{1}, y_{1}), (x_{2}, y_{2}), ...,(x_{N}, y_{N}) \right \}</script><p>其中,每个样本点由实例与标记组成.实例<script type="math/tex">x_{i}\in \chi \subseteq R^n</script>,标记<script type="math/tex">y_{i}\in \gamma =\left \{ -1, 1 \right \}</script>, <script type="math/tex">\chi</script>是实例空间,<script type="math/tex">\gamma</script>是标记集合.AdaBoost利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器<em>线性</em>组合成为一个强分类器。</p>
<h3 id="算法1-AdaBoost"><a href="#算法1-AdaBoost" class="headerlink" title="算法1(AdaBoost)"></a>算法1(AdaBoost)</h3><p>输入：训练数据集<script type="math/tex">T=\left \{ (x_{1}, y_{1}), (x_{2}, y_{2}), ...,(x_{N}, y_{N}) \right \}</script>,其中<script type="math/tex">x_{i}\in \chi \subseteq R^n</script>，<script type="math/tex">\gamma \in \left \{ -1, +1 \right \}</script>;弱学习算法：<br>输出：最终分类器G(x)。</p>
<ul>
<li>(1)初始化训练数据的权值分布 <script type="math/tex; mode=display">D_{1} = (w_{11},...,w_{1i},...,w{iN}), w_{1i}=\frac{1}{N}, i=1,2,...,N</script></li>
<li>(2)对m=1,2,…M(m代表迭代次数，或弱分类器的个数)<ul>
<li>(a)使用具有权值分布<script type="math/tex">D_{m}</script>的训练数据集学习，得到基本分类器<script type="math/tex; mode=display">G_{m}(x): \chi\rightarrow \left \{ -1,+1 \right \}</script></li>
<li>(b)计算<script type="math/tex">G_{m}(x)</script>在训练数据集上的分类误差率<script type="math/tex; mode=display">e_{m} = \sum _{i=1}^{N} P(G_{m}(x_{i})\neq y_{i})=\sum _{i=1}^{N}w_{mi}I(G_{m}(x_{i})\neq y_{i})</script></li>
<li>(c)计算<script type="math/tex">G_{m}(x)</script>的系数<script type="math/tex; mode=display">\alpha _{m} = \frac{1}{2}\log \frac {1-e_{m}}{e_{m}}</script>这里的对数是自然对数</li>
<li>(d)更新训练数据集的权值分布<script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1,i} = \frac {w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))</script>这里，<script type="math/tex">Z_{m}</script>是规范化因子<script type="math/tex; mode=display">Z_{m} = \sum _{i=1}^{N} w_{mi}exp(-\alpha _{m} y_{i} G_{m}x_{i})</script>它使<script type="math/tex">D_{m+1}</script>成为一个概率分布，<script type="math/tex">\sum _{i}^{N} w_{m+1,i}=1</script></li>
</ul>
</li>
<li>(3)构建基本分类器的线性组合<script type="math/tex; mode=display">f(x) = \sum _{m=1}^{M} \alpha_{m}G_{m}(x)</script>得到最终的分类器<script type="math/tex; mode=display">G(x)=sign(f(x))=sign( \sum _{m=1}^{M} \alpha_{m}G_{m}(x))</script></li>
</ul>
<h2 id="AdaBoost算法的训练误差分析"><a href="#AdaBoost算法的训练误差分析" class="headerlink" title="AdaBoost算法的训练误差分析"></a>AdaBoost算法的训练误差分析</h2><p>AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差，关于这个问题有下面的定理：</p>
<h3 id="定理1-AdaBoost算法最终分类器的训练误差上界为"><a href="#定理1-AdaBoost算法最终分类器的训练误差上界为" class="headerlink" title="定理1 AdaBoost算法最终分类器的训练误差上界为"></a>定理1 AdaBoost算法最终分类器的训练误差上界为</h3><script type="math/tex; mode=display">\frac {1}{N}\sum _{i=1}^{N}I(G(x_{i})\neq y_{i})\leq \frac {1}{N} \sum _{i}exp(-y_{i}f(x_{i}))=\prod _{m}Z_{m}</script><p>这里G(x), f(x)和<script type="math/tex">Z_{m}</script>有上面的式子可以得到。<br>    证明： 当<script type="math/tex">G(x_{i}) \neq y_{i}</script>时，<script type="math/tex">y_{i}f(x_{i}) < 0</script>,因此<script type="math/tex">exp(-y_{i}f(x_{i})) \geqslant 1</script>由此可以直接证明前半部分。<br>    后半部分的推导要用到<script type="math/tex">Z_{m}</script>的定义式以及更新<script type="math/tex">w_{m+1,i}</script>变形式</p>
<script type="math/tex; mode=display">w_{m,i}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))=Z_{m}w_{m+1,i}</script><p>推导如下:<br>\begin{align}<br>\frac{1}{N}\sum _{i}exp(-y_{i}f(x_{i}))\\\\<br>=\frac {1}{N}\sum _{i}(exp(-\sum _{m=1}^{M} \alpha_{m}y_{i}G_{m}(x_{i})))\\\\<br>=\sum _{i} w_{1i} \prod _{m=1}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{i} w_{1i} exp(-\alpha _{1} y_{i} G_{1}(x_{i})) \prod _{m=2}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{i} Z_{1} w_{2i} \prod _{m=2}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=Z_{1} \sum _{i} w_{2i} exp(-\alpha _{2} y_{i} G_{2}(x_{i})) \prod _{m=3}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=Z_{1} Z_{2} \sum _{i} w_{3i} exp(-\alpha _{3} y_{i} G_{3}(x_{i})) \prod _{m=4}^{M} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=…\\\\<br>=Z_{1} Z_{2} … Z_{M} \sum _{i} w_{m+1,i}\\\\<br>=\prod _{m=1}^{M} Z_{m}\\\\<br>\end{align}<br>这一定理说明，可以在每一轮选取适当的<script type="math/tex">G_{m}</script>使得<script type="math/tex">Z_{m}</script>最小，从而使训练误差下降最快。</p>
<h3 id="二分类问题AdaBoost的训练误差界"><a href="#二分类问题AdaBoost的训练误差界" class="headerlink" title="二分类问题AdaBoost的训练误差界"></a>二分类问题AdaBoost的训练误差界</h3><script type="math/tex; mode=display">\prod _{m=1}^{M} Z_{m}= \prod _{m=1}^{M}(2\sqrt {e_{m}(1-e_{m})})=\prod _{m=1}^{M} \sqrt {(1-4\gamma _{m}^{2})} \leq exp(-2\sum _{m=1}^{M} \gamma _{m}^{2})</script><p>这里，<script type="math/tex">\gamma _{m}=\frac{1}{2}-e_{m}</script></p>
<script type="math/tex; mode=display">\alpha _{m} = \frac{1}{2} \log \frac {1-e_{m}}{e_{m}}</script><p>\begin{align}<br>Z_{m}=\sum _{i=1}^{N} w_{mi} exp(-\alpha_{m} y_{i} G_{m}(x_{i}))\\\\<br>=\sum _{y_{i}=G_{m}(x_{i})} w_{mi}e^{-\alpha_{m}} + \sum _{y_{i} \neq G_{m}(x_{i})} w_{mi}e^{\alpha_{m}}\\\\<br>=(1-e_{m})e^{-\alpha_{m}}+e_{m}e^{\alpha_{m}}\\\\<br>=2 \sqrt{e_{m}(1-e_{m})} = \sqrt {1-4\gamma_{m}^{2}}<br>\end{align}<br>至于不等式:</p>
<script type="math/tex; mode=display">\prod _{m=1}^{M} \sqrt{(1-4 \gamma_{m}^{2})} \leq exp(-2 \sum _{m=1}^{M} \gamma _{m}^{2})</script><p>则可先由<script type="math/tex">e^{x}</script>和<script type="math/tex">\sqrt{1-x}</script>在点x=0的泰勒展开式推出不等式<script type="math/tex">\sqrt{(1-4\gamma_{m}^{2})} \leq exp(-2 \gamma _{m}^{2})</script></p>
<h2 id="AdaBoost算法的解释"><a href="#AdaBoost算法的解释" class="headerlink" title="AdaBoost算法的解释"></a>AdaBoost算法的解释</h2><p>AdaBoost算法还有另一个解释,即可以认为AdaBoost算法是模型为加法模型,损失函数为指数函数,学习算法为前向分步算法时的二分类学习方法.</p>
<h3 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h3><p>考虑加法模型(additive model)</p>
<script type="math/tex; mode=display">f(x)=\sum _{m=1}^{M} \beta _{m} b(x;\gamma_{m})</script><p>其中,<script type="math/tex">b(x;\gamma _{m})</script>为基函数,<script type="math/tex">\gamma _{m}</script>为基函数的系数.显然AdaBoost是一个加法模型<br>在给定训练数据集以及损失函数L(y,f(x))的条件下,学习加法模型f(x)成为经验风险极小化即损失函数极小化问题:</p>
<script type="math/tex; mode=display">\underset{\beta _{m}, \gamma _{m}}{\text{min}} \sum _{m=1}^{N} L(y_{i}, \sum _{m=1}^{M} \beta_{m}b(x_{i};\gamma_{m}))</script><p>通常这是一个复杂的优化问题,前向分步算法(forward stagewise algorithm)求解这一优化问题的想法是:因为学习模型是加法模型,如果能够从前向后,每一步只学习一个基函数及其系数,逐步逼近优化目标函数,那么就可以简化优化的复杂度.具体,每步只需要优化如下损失函数:</p>
<script type="math/tex; mode=display">\underset{\beta , \gamma }{\text{min}} \sum _{i=1}^{N} L(y_{i}, \beta b(x_{i};\gamma))</script><p>前向分步算法<br>输入:训练数据集<script type="math/tex">T=\left \{ (x_{1}, y_{1}), (x_{2}, y_{2}), ...,(x_{N}, y_{N}) \right \}</script>, <script type="math/tex">x_{i} \in \chi \subseteq R^N</script>, <script type="math/tex">y_{i} \in \gamma = \left \{-1, +1 \right \}</script>.损失函数为L(y,f(x))和基函数集合<script type="math/tex">\left \{ b(x; \gamma) \righ \}</script><br>输出:加法模型f(x)</p>
<ul>
<li>(1)初始化<script type="math/tex">f_{0}(x)=0</script></li>
<li>(2)对m=1,2…,M<br>-(a)极小化损失函数<script type="math/tex; mode=display">(\beta _{m}, \gamma _{m}) = arg \underset{\beta, \gamma }{\text{min}} \sum _{i=1}^{N} L(y_{i}, f_{m-1}(x_{xi})+\beta b(x_{i}; \gamma))</script>得到参数<script type="math/tex">\beta _{m}</script>, <script type="math/tex">\gamma _{m}</script><ul>
<li>(b)更新<script type="math/tex; mode=display">f_{m}(x) = f_{m-1}(x) + \beta_{m}b(x; \gamma_{m})</script></li>
</ul>
</li>
<li>(3)得到加法模型<script type="math/tex; mode=display">f(x)=f_{M}(x) = \sum _{m=1}{M} \beta _{m}b(x;\gamma_{m})</script>这样,前向分步算法同时求解从m=1到M的所有参数<script type="math/tex">\beta_{m}</script>, <script type="math/tex">\gamma_{m}</script>的优化问题简化为逐次求解各个<script type="math/tex">\beta _{m}</script>, <script type="math/tex">\gamma_{m}</script>的优化问题.<h3 id="向前分步算法与AdaBoost"><a href="#向前分步算法与AdaBoost" class="headerlink" title="向前分步算法与AdaBoost"></a>向前分步算法与AdaBoost</h3><a href="https://blog.csdn.net/v_JULY_v/article/details/40718799" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/40718799</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/08/AdaBoost/" data-id="cjo9si38n0002tyku41ysqfrg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/10/19/信息熵/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">信息熵</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/08/AdaBoost/">AdaBoost</a>
          </li>
        
          <li>
            <a href="/2018/10/19/信息熵/">信息熵</a>
          </li>
        
          <li>
            <a href="/2018/10/10/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>